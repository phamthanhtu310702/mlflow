{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.integration.mlflow import mlflow_mixin \n",
    "import mlflow\n",
    "import flash\n",
    "from flash.core.data.utils import download_data\n",
    "from flash.image import ImageClassificationData, ImageClassifier\n",
    "\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export env variable to access the port when using docker. \n",
    "# otherwise, use the default mlflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray tune need a trainable function to pass to tune.run\n",
    "# A required input is config.\n",
    "# Ray Tune provides a wrapper function, \n",
    "# called tune.with_parameters, \n",
    "# which allows you to pass along additional \n",
    "# arbitrary parameters and objects\n",
    "\n",
    "\n",
    "@mlflow_mixin\n",
    "def finetuning_dl_model(config, data_dir = None, num_epochs = 3, num_gpus = 0):\n",
    "    mlflow.pytorch.autolog()\n",
    "    # datamodule: batch_size tuned\n",
    "    datamodule = ImageClassificationData.from_files()\n",
    "\n",
    "    # classifier_model: optimizer, lr, backbone tuned\n",
    "    classifier_model = ImageClassifier()    \n",
    "\n",
    "    metrics = {\"loss\": \"val_cross_entropy\", \"f1\": \"val_f1\"}\n",
    "\n",
    "    # Tune Ray reports the score of metrics after every trials\n",
    "    trainer = flash.Trainer(max_epochs = num_epochs,\n",
    "                            gpus = num_gpus,\n",
    "                            progress_bar_refresh_rate=0, \n",
    "                            callbacks = [TuneReportCallback(metrics,on= 'validation_end')])\n",
    "    trainer.finetune(classifier_model, datamodule, strategy= config['finetuning_strategies'])\n",
    "\n",
    "    # batch_size hyperparameter is not automatically captured by autologging\n",
    "    mlflow.log_param('batch_size', config['batch_size'])\n",
    "    mlflow.log_param('back_bone', config['back_bone'])\n",
    "    # ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the trainable function to tune.with_parameters\n",
    "\n",
    "\n",
    "\n",
    "def run_hpo_dl_model(num_samples=10,\n",
    "                     num_epochs=3,\n",
    "                     gpus_per_trial=0,\n",
    "                     tracking_uri=None,\n",
    "                     experiment_name=\"hpo-tuning\"):\n",
    "    \n",
    "    data_dir = 'data'\n",
    "\n",
    "    # Set the MLflow experiment, or create it if it does not exist.\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # create config\n",
    "    config = {\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([1, 3, 6]),\n",
    "        \"foundation_model\": \"resnet50\",\n",
    "        \"finetuning_strategies\": \"freeze\",\n",
    "        \"optimizer_type\": \"Adam\",\n",
    "        \"mlflow\": {\n",
    "            \"experiment_name\": experiment_name,\n",
    "            \"tracking_uri\": mlflow.get_tracking_uri()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the trainable function to pass to tune.run() - avtivate HPO Ray tune\n",
    "    trainable = tune.with_parameters(finetuning_dl_model,data_dir,num_epochs, gpus_per_trial)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        trainable,\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 1,\n",
    "            \"gpu\": gpus_per_trial\n",
    "        },\n",
    "        metric=\"f1\",\n",
    "        mode=\"max\",\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        name=\"hpo_tuning_dl_model\")\n",
    "    \n",
    "    print(\"Best hyperparameters found were: %s\", analysis.best_config)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ffa61d47449aaf04ff74c970d58e2f8126d8497d1122c0c3ba55342874a97e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
